"""
LLM utilities for prompting and output parsing.

This module contains helpers used in:
- zero-shot classification
- few-shot classification
- chain-of-thought prompting

@author José Antonio García-Díaz <joseantonio.garcia8@um.es>
@author Ronghao Pan <ronghao.pan@um.es>
@author Rafael Valencia-García <valencia@um.es>
"""

from typing import List, Optional, Tuple, Dict


def build_chat_prompt (tokenizer, user_text: str, chat_history: Optional[List[Tuple[str, str]]] = None):
    """
    This helper builds a chat-style prompt using the model's official chat template.

    Many instruction-tuned LLMs expect a specific formatting (roles + special tokens).
    Using apply_chat_template ensures the prompt matches the model's training format.

    Args:
        tokenizer: Hugging Face tokenizer (must support apply_chat_template).
        user_text (str): The new user message (already includes the task prompt).
        chat_history (Optional[List[Tuple[str, str]]]): Optional list of (user, assistant)
            messages to include before the current user message.

    Returns:
        str: Formatted prompt string ready to be tokenized.
    """
    conversation: List[Dict[str, str]] = []

    if chat_history is not None:
        for user_msg, assistant_msg in chat_history:
            conversation.append ({"role": "user", "content": user_msg})
            conversation.append ({"role": "assistant", "content": assistant_msg})

    conversation.append ({"role": "user", "content": user_text})

    return tokenizer.apply_chat_template (
        conversation,
        tokenize = False,
        add_generation_prompt = True
    )


def extract_generated_text (tokenizer, inputs, decoded: str) -> str:
    """
    This helper tries to keep only the completion (without reprinting the prompt).

    Some decoder-only LLMs return the full text: [PROMPT + COMPLETION].
    We decode the input_ids and remove them from the output as a best-effort approach.

    Args:
        tokenizer: Model tokenizer.
        inputs: Tokenized inputs that contain input_ids.
        decoded (str): Full decoded output string.

    Returns:
        str: Best-effort completion string.
    """
    prompt_decoded = tokenizer.decode (
        inputs["input_ids"][0],
        skip_special_tokens = True
    ).strip ()

    out = decoded.strip ()

    if out.startswith (prompt_decoded):
        gen = out[len (prompt_decoded):].strip ()
        return gen if gen else out

    return out


def normalize_binary_label (
    raw: str,
    positive_label: str = "Positive",
    negative_label: str = "Negative",
) -> str:
    """
    This helper extracts a binary label (e.g., Positive/Negative) robustly.

    The function is case-insensitive and tolerant to additional text.
    It returns the canonical labels exactly as provided in arguments.

    Args:
        raw (str): Raw output string generated by the model.
        positive_label (str): Canonical positive label (e.g., "Positive").
        negative_label (str): Canonical negative label (e.g., "Negative").

    Returns:
        str: positive_label, negative_label, or "Classification not found".
    """
    txt = raw.strip ().lower ()

    pos_key = positive_label.strip ().lower ()
    neg_key = negative_label.strip ().lower ()
    
    
    # If both appear, select the last occurrence (often the final decision)
    if pos_key in txt and neg_key in txt:
        last_pos = txt.rfind (pos_key)
        last_neg = txt.rfind (neg_key)
        return positive_label if last_pos > last_neg else negative_label

    if pos_key in txt:
        return positive_label

    if neg_key in txt:
        return negative_label

    # Fallback: first line exact match
    first_line = raw.strip ().splitlines ()[0].strip ()
    if first_line in {positive_label, negative_label}:
        return first_line

    return "Classification not found"
